\section{Background}

\subsection{Reinforcement Learning}

Reinforcement Learning (RL) is a framework in which an agent interacts with an environment and learns a policy to maximize cumulative rewards.
The agent observes the state of the environment, takes actions, and recieves rewards based on those actions.
This process is formalized as a Markov Decision Process (MDP) \cite{MDP}, which provides a formal structure for modeling decision-making problems.
An MDP is defined by a tuple $\langle \mathcal{S}, \mathcal{A}, P, R, \gamma \rangle$, where $\mathcal{S}$ is the set of states, $\mathcal{A}$ is the set of actions, $P$ is the state transition probability function, $R$ is the reward function, and $\gamma \in [0, 1)$ is the discount factor.
In this document, we consider a finite-horizon setting and use the undiscounted return.
The objective of RL is to find an optimal policy $\pi^*$ that maximizes the expected cumulative reward, defined as:
\begin{equation}
    \begin{aligned}
        \theta^* &= \arg\max_\theta J(\theta) \\
        J(\theta) &= \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum^T_{t = 0} r_t \right]
    \end{aligned}
\end{equation}
The policy $\pi_\theta$ is assumed to be differentiable function parameterized by $\theta$, denoted as $\pi_\theta(a|s)$, which represents the probability of taking action a given state $s$.
The expectation $\mathbb{E}_{\tau \sim \pi_\theta}$ is taken over the trajectories $\tau  = (s_0, a_0, r_0, s_1, a_1, r_1, \cdots, s_T)$ generated by following the policy $\pi_\theta$.

\subsection{Constrained Reinforcement Learning}

Constrained Reinforcement Learning (CRL) extends the standard RL framework by incorporating constraints into the learning process.
CRL is formalized as a Constrained Markov Decision Process (CMDP) \cite{CMDP}, which is defined by a tuple $\langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \mathcal{C}, \gamma \rangle$.
Here, $\mathcal{C}$ is the cost functions associated with the constraints.
The feasible policy set in a CMDP is given by:
\begin{equation}
  \Pi_C = \{\pi: J_{c_i}(\pi) \leq d_i, \quad i = 1, \cdots, k\}
\end{equation}
where $J_{c_i}(\pi)$ is a cost-based constraint function defined the expected cumulative cost, and $d_i$ is the threshold for the $i$-th constraint.
The objective of CRL is to find an optimal policy that maximizes the expected cumulative reward while satisfying the constraints.
In the context of policy gradient methods, the constrained optimization problem can be formulated as follows:
\begin{equation} \label{chap1:eq:crl-objective}
  \begin{aligned}
    \theta^* &= \arg\max_\theta J(\theta) \\
    J(\theta) &= \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum^T_{t = 0} r_t \right] \; \text{subject to} \; \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum^T_{t = 0} c_t \right] \leq d
  \end{aligned}
\end{equation}

\subsubsection{Lagrangian Method}

Constrained optimization problem defined in Eq. \ref{chap1:eq:crl-objective} can be solved using various methods.
In this thesis, however, we consider only the Lagrangian method.
By applying Lagrangian relaxation, the constrained optimization problem can be transformed into an unconstrained optimization problem, in which the constraint is incorporated into the objective function using a Lagrange multiplier $\lambda$.
\begin{equation}
  \begin{aligned}
    \theta^* &= \arg\max_\theta \mathcal{L}(\theta, \lambda) \\
    \mathcal{L}(\theta, \lambda) &= \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum^T_{t = 0} r_t \right] - \lambda \left( \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum^T_{t = 0} c_t \right] - d \right)
  \end{aligned}
\end{equation}
The Lagrange multiplier $\lambda$ is a non-negative scalar that adjusts the trade-off between maximizing the expected cumulative reward and satisfying the constraints.

\subsection{State-wise Constrained Reinforcement Learning}

State-wise Constrained Reinforcement Learning (SCRL) is a variant of CRL that imposes constraints at the state level.
CRL considers the cumulative cost over the entire trajectory, while SCRL focuses on the cost at each transition.
SCRL is formalized as a State-wise Constrained Markov Decision Process (SCMPD), it is quite similar to CMDP, but SCMDP enforces the constraint for every state action trasition satisfies a hard constraints.
The objective of SCRL is to find an optimal policy that maximizes the expected cumulative reward while satisfying the state-wise constraints.
\begin{equation}
  \begin{aligned}
    \pi^* &= \arg\max_{\pi_\theta} J(\theta) \\
    J(\theta) &= \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum^T_{t = 0} r_t \right] \; \text{subject to} \; \mathbb{E}_{\tau \sim \pi_\theta}  [c(s, a)] \leq w, \quad \forall s \in S
  \end{aligned}
\end{equation}